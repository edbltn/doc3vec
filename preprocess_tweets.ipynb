{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as db\n",
    "conn = db.connect('pol_tweets.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor();\n",
    "c.execute('SELECT t_id, handle, body FROM politics_tweets')\n",
    "tweets = c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tco_regex = re.compile(r\"[:]?\\s*http[s]?://[a-zA-Z0-9?/:.]*\\b\", re.IGNORECASE)\n",
    "#tco_regex = re.compile(r\"\\s*http[s]?://.*\\b\", re.IGNORECASE)\n",
    "\n",
    "emoji_regex = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       u\"\\U0001f926-\\U0001f937\"\n",
    "                       u\"\\U00002192\"\n",
    "                       \"]+\",flags=re.UNICODE)\n",
    "pm_regex = re.compile('p\\.m', flags=re.IGNORECASE)\n",
    "am_regex = re.compile('a\\.m', flags=re.IGNORECASE)\n",
    "apostrophe_regex = re.compile(str(chr(8217)))\n",
    "hrs_abbv_regex = re.compile('hrs\\.', flags=re.IGNORECASE)\n",
    "end_quote_regex = re.compile(u\"\\U0000201D\", flags=re.UNICODE)\n",
    "hillary_abbv_regex = re.compile(r\"(-|—)h(\\s|(illary(clinton)?)+)\", flags=re.IGNORECASE)\n",
    "maga_regex = re.compile(r\"[#]?make\\s*america\\s*great\\s*again|#maga\", flags=re.IGNORECASE)\n",
    "double_hyphens_regex = re.compile(r\"(--|——)\")\n",
    "\n",
    "#apply these BEFORE lowercasing in tokenization\n",
    "usa1_regex = re.compile(r\"(^|\\s+)U[.]?[\\s]?S[.]?[\\s]?[A]?\")\n",
    "usa2_regex = re.compile(r\"united\\s+states(\\s*of\\s+america)?\", flags=re.IGNORECASE)\n",
    "la_regex = re.compile(\"L\\.A[.]?\")\n",
    "gb_regex = re.compile(r\"((George|george)[\\sa-zA-Z.]*)?Bush('s|\\s+II)?\")\n",
    "\n",
    "number_regex = re.compile(r\"([0-9]+(,|.)?)+\")\n",
    "\n",
    "replacements = [\n",
    "            [tco_regex, ' token_hyperlink '],\n",
    "            [emoji_regex, ''],\n",
    "            [pm_regex, ' pm '],\n",
    "            [am_regex, ' am '],\n",
    "            [apostrophe_regex, str(chr(39))],\n",
    "            [hrs_abbv_regex, ' hours '],\n",
    "            [end_quote_regex, str(chr(34))],\n",
    "            [maga_regex, ' token_maga '],\n",
    "            [usa1_regex, \" token_unitedstates \"],\n",
    "            [usa2_regex, \" token_unitedstates \"],\n",
    "            [hillary_abbv_regex, \" token_quotehillary \"],\n",
    "            [gb_regex, \" token_georgebush \"],\n",
    "            [la_regex, \" token_losangeles \"],\n",
    "            [double_hyphens_regex, \" -- \"]\n",
    "]\n",
    "\n",
    "removed_punc = [symb for symb in string.punctuation]+['...']+['—']\n",
    "removed_punc.remove('?')\n",
    "removed_punc.remove('!')\n",
    "removed_punc.remove('&')\n",
    "\n",
    "\n",
    "usernames_regex = re.compile(r\"@[a-zA-Z_.0-9]+\")\n",
    "hashtags_regex = re.compile(r\"#[a-zA-Z_.0-9]+\")\n",
    "list_users = [];\n",
    "list_hashtags = [];\n",
    "for r in tweets:\n",
    "    list_users.extend(usernames_regex.findall(r[2]))\n",
    "    list_hashtags.extend(hashtags_regex.findall(r[2]))\n",
    "    \n",
    "[users, ucounts] = np.unique(list_users, return_counts=True)\n",
    "[hashtags, hcounts] = np.unique(list_hashtags, return_counts=True)\n",
    "users = np.column_stack((users, ucounts)).tolist()\n",
    "hashtags = np.column_stack((hashtags, hcounts)).tolist()\n",
    "users = sorted(users, key= lambda x: int(x[1]), reverse=True)\n",
    "hashtags = sorted(hashtags, key= lambda x: int(x[1]), reverse=True)\n",
    "\n",
    "min_user_references = 10\n",
    "min_hashtag_references = 10\n",
    "\n",
    "frequent_users = [u[0].lower() for u in users if int(u[1]) > min_user_references]\n",
    "frequent_hashtags = [h[0].lower() for h in hashtags if int(h[1]) > min_hashtag_references]\n",
    "\n",
    "twtok = TweetTokenizer(preserve_case=False)\n",
    "split_tweets = []\n",
    "\n",
    "for i, tweet in enumerate(tweets):\n",
    "    line = list(tweet)\n",
    "    line.append(tweet[2])\n",
    "    for ctrlr in replacements:\n",
    "        line[3] = ctrlr[0].sub(ctrlr[1], line[3])\n",
    "        \n",
    "    line.append(twtok.tokenize(line[3])) #add fourth column of tokenized and lowercased tweet\n",
    "    line[4] = [tok for tok in line[4] if tok not in removed_punc]\n",
    "    for j, tok in enumerate(line[4]):\n",
    "        if number_regex.match(tok) != None:\n",
    "            line[4][j] = 'token_number'\n",
    "        if usernames_regex.match(tok) and tok not in frequent_users:\n",
    "            line[4][j] = 'token_rare_user'\n",
    "        if hashtags_regex.match(tok) and tok not in frequent_hashtags:\n",
    "            line[4][j] = 'token_rare_hashtag'\n",
    "    split_tweets.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['780859319319400448',\n",
       " 'realDonaldTrump',\n",
       " 'Great afternoon in Little Havana with Hispanic community leaders. Thank you for your support! #ImWithYou https://t.co/vxWZ2tyJTF',\n",
       " 'Great afternoon in Little Havana with Hispanic community leaders. Thank you for your support! #ImWithYou token_hyperlink ',\n",
       " ['great',\n",
       "  'afternoon',\n",
       "  'in',\n",
       "  'little',\n",
       "  'havana',\n",
       "  'with',\n",
       "  'hispanic',\n",
       "  'community',\n",
       "  'leaders',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'your',\n",
       "  'support',\n",
       "  '!',\n",
       "  '#imwithyou',\n",
       "  'token_hyperlink']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'261192751935279104'\n",
    "#[r for r in split_tweets if r[0] == '830047626414477312']\n",
    "split_tweets[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['765247533727834112',\n",
       " 'HillaryClinton',\n",
       " '“She’s strong. She’s respected. She’s admired. There’s nothing that she doesn’t understand about America’s place in the world.\" —@JoeBiden',\n",
       " '“She\\'s strong. She\\'s respected. She\\'s admired. There\\'s nothing that she doesn\\'t understand about America\\'s place in the world.\" —@JoeBiden',\n",
       " ['“',\n",
       "  \"she's\",\n",
       "  'strong',\n",
       "  \"she's\",\n",
       "  'respected',\n",
       "  \"she's\",\n",
       "  'admired',\n",
       "  \"there's\",\n",
       "  'nothing',\n",
       "  'that',\n",
       "  'she',\n",
       "  \"doesn't\",\n",
       "  'understand',\n",
       "  'about',\n",
       "  \"america's\",\n",
       "  'place',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  '@joebiden']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tweets[1125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
